{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e428f63-26e2-456e-95ec-aa29ddd00628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TOYOTA GR CUP - MASTER DATA PROCESSING PIPELINE\n",
      "======================================================================\n",
      "\n",
      "Tracks to process: 7\n",
      "Expected total races: 14\n",
      "Expected drivers: ~100+ driver-race combinations\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Processing: Sebring - Race 1\n",
      "======================================================================\n",
      "Loaded: 14,216,998 telemetry rows\n",
      "Files: sebring_telemetry_R1.csv, sebring_lap_time_R1.csv\n",
      "    Using 'value' column from lap_time file\n",
      "Clean laps: 352 (filtered 109 outliers)\n",
      "Drivers: 20\n",
      "  Processed 5/20 drivers...\n",
      "  Processed 10/20 drivers...\n",
      "‚úÖ Successfully processed 14 drivers with 235 corners\n",
      "\n",
      "======================================================================\n",
      "Processing: Sebring - Race 2\n",
      "======================================================================\n",
      "Loaded: 2,394,340 telemetry rows\n",
      "Files: sebring_telemetry_R2.csv, sebring_lap_time_R2.csv\n",
      "    Using 'value' column from lap_time file\n",
      "Clean laps: 277 (filtered 150 outliers)\n",
      "Drivers: 19\n",
      "‚úÖ Successfully processed 0 drivers with 0 corners\n",
      "\n",
      "======================================================================\n",
      "Processing: Sonoma - Race 1\n",
      "======================================================================\n",
      "Loaded: 27,476,808 telemetry rows\n",
      "Files: sonoma_telemetry_R1.csv, sonoma_lap_time_R1.csv\n",
      "    Using 'value' column from lap_time file\n",
      "Clean laps: 260 (filtered 786 outliers)\n",
      "Drivers: 31\n",
      "  Processed 5/31 drivers...\n",
      "  Processed 10/31 drivers...\n",
      "  Processed 15/31 drivers...\n",
      "  Processed 20/31 drivers...\n",
      "‚úÖ Successfully processed 24 drivers with 391 corners\n",
      "\n",
      "======================================================================\n",
      "Processing: Sonoma - Race 2\n",
      "======================================================================\n",
      "Loaded: 13,616,947 telemetry rows\n",
      "Files: sonoma_telemetry_R2.csv, sonoma_lap_time_R2.csv\n",
      "    Using 'value' column from lap_time file\n",
      "Clean laps: 370 (filtered 317 outliers)\n",
      "Drivers: 31\n",
      "  Processed 5/31 drivers...\n",
      "  Processed 10/31 drivers...\n",
      "  Processed 15/31 drivers...\n",
      "  Processed 20/31 drivers...\n",
      "  Processed 25/31 drivers...\n",
      "  Processed 30/31 drivers...\n",
      "‚úÖ Successfully processed 31 drivers with 486 corners\n",
      "\n",
      "======================================================================\n",
      "Processing: Barber - Race 1\n",
      "======================================================================\n",
      "Loaded: 11,556,519 telemetry rows\n",
      "Files: R1_barber_telemetry_data.csv, R1_barber_lap_time.csv\n",
      "    No 'value' column, trying lap_start/lap_end method...\n",
      "    Calculating lap times from timestamps...\n",
      "    Calculated 666 lap times\n",
      "Clean laps: 57 (filtered 609 outliers)\n",
      "Drivers: 20\n",
      "  Processed 5/20 drivers...\n",
      "  Processed 10/20 drivers...\n",
      "‚úÖ Successfully processed 13 drivers with 172 corners\n",
      "\n",
      "======================================================================\n",
      "Processing: Barber - Race 2\n",
      "======================================================================\n",
      "Loaded: 11,749,604 telemetry rows\n",
      "Files: R2_barber_telemetry_data.csv, R2_barber_lap_time.csv\n",
      "    No 'value' column, trying lap_start/lap_end method...\n",
      "    Calculating lap times from timestamps...\n",
      "    Calculated 709 lap times\n",
      "Clean laps: 3 (filtered 706 outliers)\n",
      "Drivers: 3\n",
      "‚úÖ Successfully processed 1 drivers with 10 corners\n",
      "\n",
      "======================================================================\n",
      "Processing: Indianapolis - Race 1\n",
      "======================================================================\n",
      "Loaded: 21,454,865 telemetry rows\n",
      "Files: R1_indianapolis_motor_speedway_telemetry.csv, R1_indianapolis_motor_speedway_lap_time.csv\n",
      "    No 'value' column, trying lap_start/lap_end method...\n",
      "    Calculating lap times from timestamps...\n",
      "    Calculated 906 lap times\n",
      "Clean laps: 139 (filtered 767 outliers)\n",
      "Drivers: 25\n",
      "  Processed 5/25 drivers...\n",
      "  Processed 10/25 drivers...\n",
      "  Processed 15/25 drivers...\n",
      "‚úÖ Successfully processed 17 drivers with 220 corners\n",
      "\n",
      "======================================================================\n",
      "Processing: Indianapolis - Race 2\n",
      "======================================================================\n",
      "Loaded: 23,064,086 telemetry rows\n",
      "Files: R2_indianapolis_motor_speedway_telemetry.csv, R2_indianapolis_motor_speedway_lap_time.csv\n",
      "    No 'value' column, trying lap_start/lap_end method...\n",
      "    Calculating lap times from timestamps...\n",
      "    Calculated 752 lap times\n",
      "Clean laps: 276 (filtered 476 outliers)\n",
      "Drivers: 25\n",
      "  Processed 5/25 drivers...\n",
      "  Processed 10/25 drivers...\n",
      "  Processed 15/25 drivers...\n",
      "  Processed 20/25 drivers...\n",
      "‚úÖ Successfully processed 20 drivers with 273 corners\n",
      "\n",
      "======================================================================\n",
      "Processing: VIR - Race 1\n",
      "======================================================================\n",
      "Loaded: 11,401,181 telemetry rows\n",
      "Files: R1_vir_telemetry_data.csv, vir_lap_time_R1.csv\n",
      "    Using 'value' column from lap_time file\n",
      "Clean laps: 333 (filtered 150 outliers)\n",
      "Drivers: 21\n",
      "  Processed 5/21 drivers...\n",
      "  Processed 10/21 drivers...\n",
      "  Processed 15/21 drivers...\n",
      "  Processed 20/21 drivers...\n",
      "‚úÖ Successfully processed 20 drivers with 272 corners\n",
      "\n",
      "======================================================================\n",
      "Processing: VIR - Race 2\n",
      "======================================================================\n",
      "Loaded: 11,753,289 telemetry rows\n",
      "Files: R2_vir_telemetry_data.csv, vir_lap_time_R2.csv\n",
      "    Using 'value' column from lap_time file\n",
      "Clean laps: 333 (filtered 150 outliers)\n",
      "Drivers: 21\n",
      "  Processed 5/21 drivers...\n",
      "  Processed 10/21 drivers...\n",
      "  Processed 15/21 drivers...\n",
      "‚úÖ Successfully processed 18 drivers with 252 corners\n",
      "\n",
      "======================================================================\n",
      "Processing: Road America - Race 1\n",
      "======================================================================\n",
      "Loaded: 8,981,264 telemetry rows\n",
      "Files: R1_road_america_telemetry_data.csv, road_america_lap_time_R1.csv\n",
      "    Using 'value' column from lap_time file\n",
      "Clean laps: 88 (filtered 133 outliers)\n",
      "Drivers: 24\n",
      "  Processed 5/24 drivers...\n",
      "  Processed 10/24 drivers...\n",
      "  Processed 15/24 drivers...\n",
      "  Processed 20/24 drivers...\n",
      "‚úÖ Successfully processed 23 drivers with 536 corners\n",
      "\n",
      "======================================================================\n",
      "Processing: Road America - Race 2\n",
      "======================================================================\n",
      "Loaded: 11,276,849 telemetry rows\n",
      "Files: R2_road_america_telemetry_data.csv, road_america_lap_time_R2.csv\n",
      "    Using 'value' column from lap_time file\n",
      "Clean laps: 329 (filtered 97 outliers)\n",
      "Drivers: 26\n",
      "  Processed 5/26 drivers...\n",
      "  Processed 10/26 drivers...\n",
      "  Processed 15/26 drivers...\n",
      "  Processed 20/26 drivers...\n",
      "  Processed 25/26 drivers...\n",
      "‚úÖ Successfully processed 26 drivers with 458 corners\n",
      "\n",
      "======================================================================\n",
      "Processing: COTA - Race 1\n",
      "======================================================================\n",
      "Loaded: 17,754,164 telemetry rows\n",
      "Files: R1_cota_telemetry_data.csv, COTA_lap_time_R1.csv\n",
      "    Using 'value' column from lap_time file\n",
      "Clean laps: 393 (filtered 238 outliers)\n",
      "Drivers: 30\n",
      "  Processed 5/30 drivers...\n",
      "  Processed 10/30 drivers...\n",
      "  Processed 15/30 drivers...\n",
      "  Processed 20/30 drivers...\n",
      "  Processed 25/30 drivers...\n",
      "‚úÖ Successfully processed 29 drivers with 512 corners\n",
      "\n",
      "======================================================================\n",
      "Processing: COTA - Race 2\n",
      "======================================================================\n",
      "Loaded: 17,223,368 telemetry rows\n",
      "Files: R2_cota_telemetry_data.csv, COTA_lap_time_R2.csv\n",
      "    Using 'value' column from lap_time file\n",
      "Clean laps: 386 (filtered 251 outliers)\n",
      "Drivers: 31\n",
      "  Processed 5/31 drivers...\n",
      "  Processed 10/31 drivers...\n",
      "  Processed 15/31 drivers...\n",
      "  Processed 20/31 drivers...\n",
      "  Processed 25/31 drivers...\n",
      "‚úÖ Successfully processed 29 drivers with 526 corners\n",
      "\n",
      "======================================================================\n",
      "PROCESSING COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Dataset Summary:\n",
      "       track   race  drivers  corners  avg_lap_time\n",
      "     Sebring Race 1       14      235    147.573638\n",
      "      Sonoma Race 1       24      391    116.766552\n",
      "      Sonoma Race 2       31      486    115.808833\n",
      "      Barber Race 1       13      172    117.997512\n",
      "      Barber Race 2        1       10    110.129000\n",
      "Indianapolis Race 1       17      220    133.752255\n",
      "Indianapolis Race 2       20      273    119.546615\n",
      "         VIR Race 1       20      272    130.199460\n",
      "         VIR Race 2       18      252    129.323365\n",
      "Road America Race 1       23      536    169.831810\n",
      "Road America Race 2       26      458    153.344225\n",
      "        COTA Race 1       29      512    150.691236\n",
      "        COTA Race 2       29      526    149.835049\n",
      "\n",
      "üìä Total Statistics:\n",
      "  - Total corner features: 4,343\n",
      "  - Unique drivers: 64\n",
      "  - Unique tracks: 7\n",
      "  - Total races: 13\n",
      "\n",
      "‚úÖ Saved: master_corner_features.csv\n",
      "‚úÖ Saved: processing_summary.csv\n",
      "\n",
      "======================================================================\n",
      "DRIVER PERFORMANCE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Saved: driver_performance_stats.csv\n",
      "\n",
      "üèÜ Top 3 Drivers per Track:\n",
      "\n",
      "Barber:\n",
      "  1. GR86-038-93: 110.13s (92th percentile)\n",
      "  2. GR86-047-21: 110.29s (85th percentile)\n",
      "  3. GR86-040-3: 112.49s (77th percentile)\n",
      "\n",
      "COTA:\n",
      "  1. GR86-006-7: 148.07s (97th percentile)\n",
      "  2. GR86-022-13: 148.10s (94th percentile)\n",
      "  3. GR86-033-46: 148.40s (90th percentile)\n",
      "\n",
      "Indianapolis:\n",
      "  1. GR86-013-80: 115.39s (96th percentile)\n",
      "  2. GR86-040-3: 115.79s (92th percentile)\n",
      "  3. GR86-012-8: 116.35s (88th percentile)\n",
      "\n",
      "Road America:\n",
      "  1. GR86-015-31: 146.18s (96th percentile)\n",
      "  2. GR86-016-55: 151.58s (93th percentile)\n",
      "  3. GR86-037-03: 151.59s (89th percentile)\n",
      "\n",
      "Sebring:\n",
      "  1. GR86-033-46: 145.65s (93th percentile)\n",
      "  2. GR86-036-98: 147.00s (86th percentile)\n",
      "  3. GR86-049-88: 147.17s (79th percentile)\n",
      "\n",
      "Sonoma:\n",
      "  1. GR86-003-017: 115.00s (95th percentile)\n",
      "  1. GR86-008-113: 115.00s (95th percentile)\n",
      "  3. GR86-010-016: 115.01s (90th percentile)\n",
      "\n",
      "VIR:\n",
      "  1. GR86-036-98: 117.02s (95th percentile)\n",
      "  2. GR86-033-46: 128.46s (90th percentile)\n",
      "  3. GR86-022-13: 128.49s (86th percentile)\n",
      "\n",
      "======================================================================\n",
      "GENERATING COMPARISON DATASETS\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Sebring:\n",
      "  Fastest: GR86-033-46 (145.65s)\n",
      "  Comparisons generated: 13 drivers\n",
      "\n",
      "Sonoma:\n",
      "  Fastest: GR86-003-017 (115.00s)\n",
      "  Comparisons generated: 30 drivers\n",
      "\n",
      "Barber:\n",
      "  Fastest: GR86-038-93 (110.13s)\n",
      "  Comparisons generated: 12 drivers\n",
      "\n",
      "Indianapolis:\n",
      "  Fastest: GR86-013-80 (115.39s)\n",
      "  Comparisons generated: 23 drivers\n",
      "\n",
      "VIR:\n",
      "  Fastest: GR86-036-98 (117.02s)\n",
      "  Comparisons generated: 20 drivers\n",
      "\n",
      "Road America:\n",
      "  Fastest: GR86-015-31 (146.18s)\n",
      "  Comparisons generated: 26 drivers\n",
      "\n",
      "COTA:\n",
      "  Fastest: GR86-006-7 (148.07s)\n",
      "  Comparisons generated: 30 drivers\n",
      "\n",
      "‚úÖ Saved: master_comparisons.csv (2,474 comparisons)\n",
      "\n",
      "======================================================================\n",
      "TRAINING MACHINE LEARNING MODELS\n",
      "======================================================================\n",
      "\n",
      "ML Dataset: 248 samples\n",
      "\n",
      "1. Training Random Forest...\n",
      "   R¬≤ Score: 0.899\n",
      "\n",
      "   Feature Importance:\n",
      "     avg_steering_angle  : 0.3407\n",
      "     max_brake           : 0.2127\n",
      "     corner_duration     : 0.1532\n",
      "     apex_lateral_g      : 0.1179\n",
      "     exit_throttle       : 0.0597\n",
      "     brake_duration      : 0.0426\n",
      "     apex_throttle       : 0.0372\n",
      "     min_throttle        : 0.0361\n",
      "\n",
      "2. Training Gradient Boosting...\n",
      "   R¬≤ Score: 1.000\n",
      "\n",
      "‚úÖ Saved: rf_model.pkl\n",
      "‚úÖ Saved: gb_model.pkl\n",
      "‚úÖ Saved: ml_feature_importance.csv\n",
      "\n",
      "======================================================================\n",
      "DRIVER STYLE CLUSTERING\n",
      "======================================================================\n",
      "\n",
      "Clustering 64 drivers...\n",
      "\n",
      "Cluster Characteristics:\n",
      "\n",
      "Smooth & Consistent (14 drivers):\n",
      "  Avg Brake: 43.5\n",
      "  Avg Apex Throttle: 9.1%\n",
      "  Avg Exit Throttle: 92.3%\n",
      "  Example drivers: GR86-002-002, GR86-006-007, GR86-021-86\n",
      "\n",
      "Aggressive Late-Braker (14 drivers):\n",
      "  Avg Brake: 60.2\n",
      "  Avg Apex Throttle: 5.7%\n",
      "  Avg Exit Throttle: 93.3%\n",
      "  Example drivers: GR86-002-2, GR86-003-017, GR86-008-113\n",
      "\n",
      "Early Apex Specialist (34 drivers):\n",
      "  Avg Brake: 51.4\n",
      "  Avg Apex Throttle: 5.5%\n",
      "  Avg Exit Throttle: 92.5%\n",
      "  Example drivers: GR86-004-078, GR86-004-78, GR86-006-7\n",
      "\n",
      "High-Speed Demon (2 drivers):\n",
      "  Avg Brake: 22.7\n",
      "  Avg Apex Throttle: 6.1%\n",
      "  Avg Exit Throttle: 86.0%\n",
      "  Example drivers: GR86-002-000, GR86-060-2\n",
      "\n",
      "‚úÖ Saved: driver_clusters.csv\n",
      "\n",
      "======================================================================\n",
      "EXPORT SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìÅ Files Generated:\n",
      "  1. master_corner_features.csv - All corner data from all tracks\n",
      "  2. processing_summary.csv - Track/race processing stats\n",
      "  3. driver_performance_stats.csv - Driver rankings and stats\n",
      "  4. master_comparisons.csv - All driver comparisons\n",
      "  5. ml_feature_importance.csv - ML insights\n",
      "  6. driver_clusters.csv - Driver style classifications\n",
      "  7. rf_model.pkl - Random Forest model\n",
      "  8. gb_model.pkl - Gradient Boosting model\n",
      "\n",
      "üìä Dataset Statistics:\n",
      "  Total Corners Analyzed: 4,343\n",
      "  Unique Drivers: 64\n",
      "  Tracks: 7\n",
      "  Total Comparisons: 2,474\n",
      "  ML Training Samples: 248\n",
      "\n",
      "‚úÖ ALL DATA PROCESSING COMPLETE!\n",
      "\n",
      "Next steps:\n",
      "  1. Update app.py with multi-track support\n",
      "  2. Test dashboard with new data\n",
      "  3. Add advanced visualizations\n"
     ]
    }
   ],
   "source": [
    "# 03_process_all_data.ipynb\n",
    "# Master Data Processing Pipeline - All Tracks, All Drivers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our utilities - FIXED\n",
    "from utils import get_lap_telemetry, detect_corners, extract_corner_features\n",
    "from config import TRACKS\n",
    "\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TOYOTA GR CUP - MASTER DATA PROCESSING PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTracks to process: {len(TRACKS)}\")\n",
    "print(f\"Expected total races: {len(TRACKS) * 2}\")\n",
    "print(f\"Expected drivers: ~100+ driver-race combinations\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 1: PROCESS ALL TRACKS - HANDLES BOTH LAP TIME FORMATS\n",
    "# ============================================================================\n",
    "\n",
    "all_corner_features = []\n",
    "processing_summary = []\n",
    "\n",
    "for track_name, config in TRACKS.items():\n",
    "    track_path = config['path']\n",
    "    \n",
    "    if not track_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  {track_name}: Path not found at {track_path}, skipping\")\n",
    "        continue\n",
    "    \n",
    "    for race_idx, race_prefix in enumerate(config['races'], 1):\n",
    "        try:\n",
    "            if config['has_race_dirs']:\n",
    "                race_dir = track_path / f\"Race {race_idx}\"\n",
    "            else:\n",
    "                race_dir = track_path\n",
    "            \n",
    "            if not race_dir.exists():\n",
    "                print(f\"‚ö†Ô∏è  {track_name} Race {race_idx}: Directory not found\")\n",
    "                continue\n",
    "            \n",
    "            all_files = list(race_dir.glob(\"*.csv\")) + list(race_dir.glob(\"*.CSV\"))\n",
    "            \n",
    "            telemetry_files = [f for f in all_files if 'telemetry' in f.name.lower()]\n",
    "            lap_time_files = [f for f in all_files if 'lap_time' in f.name.lower()]\n",
    "            lap_start_files = [f for f in all_files if 'lap_start' in f.name.lower()]\n",
    "            lap_end_files = [f for f in all_files if 'lap_end' in f.name.lower()]\n",
    "            \n",
    "            if not config['has_race_dirs']:\n",
    "                telemetry_files = [f for f in telemetry_files if f\"R{race_idx}\" in f.name]\n",
    "                lap_time_files = [f for f in lap_time_files if f\"R{race_idx}\" in f.name]\n",
    "                lap_start_files = [f for f in lap_start_files if f\"R{race_idx}\" in f.name]\n",
    "                lap_end_files = [f for f in lap_end_files if f\"R{race_idx}\" in f.name]\n",
    "            \n",
    "            if not telemetry_files:\n",
    "                print(f\"‚ö†Ô∏è  {track_name} Race {race_idx}: No telemetry files\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Processing: {track_name} - Race {race_idx}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            telemetry = pd.read_csv(telemetry_files[0])\n",
    "            print(f\"Loaded: {len(telemetry):,} telemetry rows\")\n",
    "            \n",
    "            # METHOD 1: Try lap_time file with 'value' column\n",
    "            if lap_time_files:\n",
    "                print(f\"Files: {telemetry_files[0].name}, {lap_time_files[0].name}\")\n",
    "                lap_times = pd.read_csv(lap_time_files[0])\n",
    "                \n",
    "                # Check if 'value' column exists\n",
    "                if 'value' in lap_times.columns:\n",
    "                    print(f\"    Using 'value' column from lap_time file\")\n",
    "                    lap_times['value'] = pd.to_numeric(lap_times['value'], errors='coerce')\n",
    "                    lap_times = lap_times.dropna(subset=['value'])\n",
    "                    \n",
    "                    max_time_raw = lap_times['value'].max()\n",
    "                    if max_time_raw > 300:\n",
    "                        lap_times['lap_time_sec'] = lap_times['value'] / 1000\n",
    "                    else:\n",
    "                        lap_times['lap_time_sec'] = lap_times['value']\n",
    "                else:\n",
    "                    # METHOD 2: Calculate from lap_start and lap_end timestamps\n",
    "                    print(f\"    No 'value' column, trying lap_start/lap_end method...\")\n",
    "                    \n",
    "                    if not lap_start_files or not lap_end_files:\n",
    "                        print(f\"    ‚ö†Ô∏è Missing lap_start or lap_end files, skipping\")\n",
    "                        continue\n",
    "                    \n",
    "                    lap_start = pd.read_csv(lap_start_files[0])\n",
    "                    lap_end = pd.read_csv(lap_end_files[0])\n",
    "                    \n",
    "                    print(f\"    Calculating lap times from timestamps...\")\n",
    "                    \n",
    "                    # Convert timestamps to datetime\n",
    "                    lap_start['timestamp'] = pd.to_datetime(lap_start['timestamp'])\n",
    "                    lap_end['timestamp'] = pd.to_datetime(lap_end['timestamp'])\n",
    "                    \n",
    "                    # Merge on vehicle_id and lap\n",
    "                    lap_times = pd.merge(\n",
    "                        lap_start[['vehicle_id', 'lap', 'timestamp']],\n",
    "                        lap_end[['vehicle_id', 'lap', 'timestamp']],\n",
    "                        on=['vehicle_id', 'lap'],\n",
    "                        suffixes=('_start', '_end')\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate lap time in seconds\n",
    "                    lap_times['lap_time_sec'] = (lap_times['timestamp_end'] - lap_times['timestamp_start']).dt.total_seconds()\n",
    "                    \n",
    "                    print(f\"    Calculated {len(lap_times)} lap times\")\n",
    "            else:\n",
    "                print(f\"    ‚ö†Ô∏è No lap time files found\")\n",
    "                continue\n",
    "            \n",
    "            # Filter reasonable lap times\n",
    "            min_time, max_time = config['typical_lap_time']\n",
    "            lap_times_clean = lap_times[\n",
    "                (lap_times['lap_time_sec'] > min_time) & \n",
    "                (lap_times['lap_time_sec'] < max_time)\n",
    "            ].copy()\n",
    "            \n",
    "            print(f\"Clean laps: {len(lap_times_clean)} (filtered {len(lap_times) - len(lap_times_clean)} outliers)\")\n",
    "            \n",
    "            if len(lap_times_clean) == 0:\n",
    "                print(f\"‚ö†Ô∏è  No valid laps - check time range {min_time}-{max_time}s\")\n",
    "                actual_times = lap_times['lap_time_sec'].dropna()\n",
    "                if len(actual_times) > 0:\n",
    "                    print(f\"    Actual range: {actual_times.min():.1f}s - {actual_times.max():.1f}s\")\n",
    "                continue\n",
    "            \n",
    "            all_drivers = lap_times_clean['vehicle_id'].unique().tolist()\n",
    "            print(f\"Drivers: {len(all_drivers)}\")\n",
    "            \n",
    "            race_features = []\n",
    "            successful_drivers = 0\n",
    "            \n",
    "            for vehicle in all_drivers:\n",
    "                driver_laps = lap_times_clean[lap_times_clean['vehicle_id'] == vehicle]\n",
    "                best_lap_info = driver_laps.nsmallest(1, 'lap_time_sec')\n",
    "                \n",
    "                if len(best_lap_info) == 0:\n",
    "                    continue\n",
    "                \n",
    "                lap_num = best_lap_info['lap'].iloc[0]\n",
    "                lap_time = best_lap_info['lap_time_sec'].iloc[0]\n",
    "                \n",
    "                lap_telem = get_lap_telemetry(telemetry, vehicle, lap_num)\n",
    "                \n",
    "                if lap_telem is None or len(lap_telem) < 100:\n",
    "                    continue\n",
    "                \n",
    "                corners = detect_corners(lap_telem)\n",
    "                \n",
    "                min_corners, max_corners = config['expected_corners']\n",
    "                if len(corners) < min_corners or len(corners) > max_corners:\n",
    "                    continue\n",
    "                \n",
    "                features = extract_corner_features(lap_telem, corners)\n",
    "                features['vehicle_id'] = vehicle\n",
    "                features['lap'] = lap_num\n",
    "                features['lap_time'] = lap_time\n",
    "                features['track'] = track_name\n",
    "                features['race'] = f\"Race {race_idx}\"\n",
    "                \n",
    "                race_features.append(features)\n",
    "                successful_drivers += 1\n",
    "                \n",
    "                if successful_drivers % 5 == 0:\n",
    "                    print(f\"  Processed {successful_drivers}/{len(all_drivers)} drivers...\")\n",
    "            \n",
    "            print(f\"‚úÖ Successfully processed {successful_drivers} drivers with {sum(len(f) for f in race_features)} corners\")\n",
    "            \n",
    "            if race_features:\n",
    "                race_df = pd.concat(race_features, ignore_index=True)\n",
    "                all_corner_features.append(race_df)\n",
    "                \n",
    "                processing_summary.append({\n",
    "                    'track': track_name,\n",
    "                    'race': f\"Race {race_idx}\",\n",
    "                    'drivers': successful_drivers,\n",
    "                    'corners': len(race_df),\n",
    "                    'avg_lap_time': race_df['lap_time'].mean()\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {track_name} Race {race_idx}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "if all_corner_features:\n",
    "    master_corners = pd.concat(all_corner_features, ignore_index=True)\n",
    "    summary_df = pd.DataFrame(processing_summary)\n",
    "    \n",
    "    print(\"Dataset Summary:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(f\"\\nüìä Total Statistics:\")\n",
    "    print(f\"  - Total corner features: {len(master_corners):,}\")\n",
    "    print(f\"  - Unique drivers: {master_corners['vehicle_id'].nunique()}\")\n",
    "    print(f\"  - Unique tracks: {master_corners['track'].nunique()}\")\n",
    "    print(f\"  - Total races: {len(processing_summary)}\")\n",
    "    \n",
    "    master_corners.to_csv('master_corner_features.csv', index=False)\n",
    "    summary_df.to_csv('processing_summary.csv', index=False)\n",
    "    print(\"\\n‚úÖ Saved: master_corner_features.csv\")\n",
    "    print(\"‚úÖ Saved: processing_summary.csv\")\n",
    "else:\n",
    "    print(\"‚ùå No data was successfully processed!\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 2: DRIVER PERFORMANCE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DRIVER PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Calculate driver statistics per track\n",
    "driver_stats = master_corners.groupby(['track', 'vehicle_id']).agg({\n",
    "    'lap_time': ['mean', 'min', 'count'],\n",
    "    'max_brake': 'mean',\n",
    "    'apex_throttle': 'mean',\n",
    "    'exit_throttle': 'mean',\n",
    "    'corner_duration': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "driver_stats.columns = ['track', 'vehicle_id', 'avg_lap', 'best_lap', 'laps_analyzed', \n",
    "                        'avg_brake', 'avg_apex_throttle', 'avg_exit_throttle', 'avg_corner_duration']\n",
    "\n",
    "# Add percentile rankings per track\n",
    "for track in driver_stats['track'].unique():\n",
    "    track_mask = driver_stats['track'] == track\n",
    "    driver_stats.loc[track_mask, 'rank'] = driver_stats.loc[track_mask, 'best_lap'].rank()\n",
    "    driver_stats.loc[track_mask, 'percentile'] = (\n",
    "        100 - (driver_stats.loc[track_mask, 'rank'] / driver_stats.loc[track_mask].shape[0] * 100)\n",
    "    )\n",
    "\n",
    "driver_stats.to_csv('driver_performance_stats.csv', index=False)\n",
    "print(\"‚úÖ Saved: driver_performance_stats.csv\")\n",
    "\n",
    "# Show top performers per track\n",
    "print(\"\\nüèÜ Top 3 Drivers per Track:\")\n",
    "for track in driver_stats['track'].unique():\n",
    "    track_data = driver_stats[driver_stats['track'] == track].nsmallest(3, 'best_lap')\n",
    "    print(f\"\\n{track}:\")\n",
    "    for idx, row in track_data.iterrows():\n",
    "        print(f\"  {int(row['rank'])}. {row['vehicle_id']}: {row['best_lap']:.2f}s ({row['percentile']:.0f}th percentile)\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 3: GENERATE COMPARISON DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING COMPARISON DATASETS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "all_comparisons = []\n",
    "\n",
    "# For each track, compare all drivers against the fastest\n",
    "for track in master_corners['track'].unique():\n",
    "    track_data = master_corners[master_corners['track'] == track]\n",
    "    \n",
    "    # Find fastest driver\n",
    "    fastest_driver = track_data.groupby('vehicle_id')['lap_time'].min().idxmin()\n",
    "    fastest_lap_time = track_data[track_data['vehicle_id'] == fastest_driver]['lap_time'].min()\n",
    "    \n",
    "    print(f\"\\n{track}:\")\n",
    "    print(f\"  Fastest: {fastest_driver} ({fastest_lap_time:.2f}s)\")\n",
    "    \n",
    "    # Get fastest driver's corner data\n",
    "    fast_corners = track_data[track_data['vehicle_id'] == fastest_driver].copy()\n",
    "    \n",
    "    # Compare all other drivers\n",
    "    other_drivers = [d for d in track_data['vehicle_id'].unique() if d != fastest_driver]\n",
    "    \n",
    "    for driver in other_drivers:\n",
    "        driver_corners = track_data[track_data['vehicle_id'] == driver].copy()\n",
    "        driver_lap_time = driver_corners['lap_time'].iloc[0]\n",
    "        \n",
    "        # Compare corner by corner\n",
    "        max_corners = min(len(fast_corners), len(driver_corners))\n",
    "        \n",
    "        for corner_num in range(1, max_corners + 1):\n",
    "            fast_c = fast_corners[fast_corners['corner_num'] == corner_num]\n",
    "            slow_c = driver_corners[driver_corners['corner_num'] == corner_num]\n",
    "            \n",
    "            if len(fast_c) == 0 or len(slow_c) == 0:\n",
    "                continue\n",
    "            \n",
    "            fast = fast_c.iloc[0]\n",
    "            slow = slow_c.iloc[0]\n",
    "            \n",
    "            all_comparisons.append({\n",
    "                'track': track,\n",
    "                'fast_driver': fastest_driver,\n",
    "                'fast_lap_time': fastest_lap_time,\n",
    "                'slow_driver': driver,\n",
    "                'slow_lap_time': driver_lap_time,\n",
    "                'time_gap': driver_lap_time - fastest_lap_time,\n",
    "                'corner': corner_num,\n",
    "                'brake_delta': slow['max_brake'] - fast['max_brake'],\n",
    "                'apex_throttle_delta': slow['apex_throttle'] - fast['apex_throttle'],\n",
    "                'duration_delta': slow['corner_duration'] - fast['corner_duration'],\n",
    "                'fast_brake': fast['max_brake'],\n",
    "                'slow_brake': slow['max_brake'],\n",
    "                'fast_apex_throttle': fast['apex_throttle'],\n",
    "                'slow_apex_throttle': slow['apex_throttle'],\n",
    "                'time_lost_sec': (slow['corner_duration'] - fast['corner_duration']) * 0.04\n",
    "            })\n",
    "    \n",
    "    print(f\"  Comparisons generated: {len(other_drivers)} drivers\")\n",
    "\n",
    "comparison_df = pd.DataFrame(all_comparisons)\n",
    "comparison_df.to_csv('master_comparisons.csv', index=False)\n",
    "print(f\"\\n‚úÖ Saved: master_comparisons.csv ({len(comparison_df):,} comparisons)\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 4: MACHINE LEARNING MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING MACHINE LEARNING MODELS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Prepare ML dataset - average corner features per lap\n",
    "ml_data = master_corners.groupby(['track', 'vehicle_id', 'lap', 'lap_time']).agg({\n",
    "    'max_brake': 'mean',\n",
    "    'brake_duration': 'mean',\n",
    "    'apex_throttle': 'mean',\n",
    "    'min_throttle': 'mean',\n",
    "    'apex_lateral_g': 'mean',\n",
    "    'exit_throttle': 'mean',\n",
    "    'corner_duration': 'mean',\n",
    "    'avg_steering_angle': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"ML Dataset: {len(ml_data)} samples\")\n",
    "\n",
    "# Features and target\n",
    "feature_cols = ['max_brake', 'brake_duration', 'apex_throttle', 'min_throttle', \n",
    "                'apex_lateral_g', 'exit_throttle', 'corner_duration', 'avg_steering_angle']\n",
    "X = ml_data[feature_cols]\n",
    "y = ml_data['lap_time']\n",
    "\n",
    "# Train Random Forest\n",
    "print(\"\\n1. Training Random Forest...\")\n",
    "rf_model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X, y)\n",
    "rf_score = rf_model.score(X, y)\n",
    "print(f\"   R¬≤ Score: {rf_score:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n   Feature Importance:\")\n",
    "for idx, row in feature_importance.iterrows():\n",
    "    print(f\"     {row['feature']:<20s}: {row['importance']:.4f}\")\n",
    "\n",
    "feature_importance.to_csv('ml_feature_importance.csv', index=False)\n",
    "\n",
    "# Train Gradient Boosting\n",
    "print(\"\\n2. Training Gradient Boosting...\")\n",
    "gb_model = GradientBoostingRegressor(n_estimators=200, max_depth=8, random_state=42)\n",
    "gb_model.fit(X, y)\n",
    "gb_score = gb_model.score(X, y)\n",
    "print(f\"   R¬≤ Score: {gb_score:.3f}\")\n",
    "\n",
    "# Save models\n",
    "with open('rf_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "with open('gb_model.pkl', 'wb') as f:\n",
    "    pickle.dump(gb_model, f)\n",
    "\n",
    "print(\"\\n‚úÖ Saved: rf_model.pkl\")\n",
    "print(\"‚úÖ Saved: gb_model.pkl\")\n",
    "print(\"‚úÖ Saved: ml_feature_importance.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 5: DRIVER CLUSTERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DRIVER STYLE CLUSTERING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Prepare clustering features\n",
    "cluster_features = ml_data.groupby('vehicle_id').agg({\n",
    "    'max_brake': 'mean',\n",
    "    'apex_throttle': 'mean',\n",
    "    'exit_throttle': 'mean',\n",
    "    'corner_duration': 'mean',\n",
    "    'apex_lateral_g': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"Clustering {len(cluster_features)} drivers...\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(cluster_features[['max_brake', 'apex_throttle', 'exit_throttle', \n",
    "                                                    'corner_duration', 'apex_lateral_g']])\n",
    "\n",
    "# K-means clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "cluster_features['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Label clusters based on characteristics\n",
    "cluster_labels = {\n",
    "    0: \"Smooth & Consistent\",\n",
    "    1: \"Aggressive Late-Braker\",\n",
    "    2: \"Early Apex Specialist\",\n",
    "    3: \"High-Speed Demon\"\n",
    "}\n",
    "\n",
    "# Analyze clusters\n",
    "print(\"\\nCluster Characteristics:\")\n",
    "for cluster_id in range(4):\n",
    "    cluster_data = cluster_features[cluster_features['cluster'] == cluster_id]\n",
    "    print(f\"\\n{cluster_labels.get(cluster_id, f'Cluster {cluster_id}')} ({len(cluster_data)} drivers):\")\n",
    "    print(f\"  Avg Brake: {cluster_data['max_brake'].mean():.1f}\")\n",
    "    print(f\"  Avg Apex Throttle: {cluster_data['apex_throttle'].mean():.1f}%\")\n",
    "    print(f\"  Avg Exit Throttle: {cluster_data['exit_throttle'].mean():.1f}%\")\n",
    "    print(f\"  Example drivers: {', '.join(cluster_data['vehicle_id'].head(3).tolist())}\")\n",
    "\n",
    "cluster_features['cluster_label'] = cluster_features['cluster'].map(cluster_labels)\n",
    "cluster_features.to_csv('driver_clusters.csv', index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Saved: driver_clusters.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 6: EXPORT SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPORT SUMMARY\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"üìÅ Files Generated:\")\n",
    "print(\"  1. master_corner_features.csv - All corner data from all tracks\")\n",
    "print(\"  2. processing_summary.csv - Track/race processing stats\")\n",
    "print(\"  3. driver_performance_stats.csv - Driver rankings and stats\")\n",
    "print(\"  4. master_comparisons.csv - All driver comparisons\")\n",
    "print(\"  5. ml_feature_importance.csv - ML insights\")\n",
    "print(\"  6. driver_clusters.csv - Driver style classifications\")\n",
    "print(\"  7. rf_model.pkl - Random Forest model\")\n",
    "print(\"  8. gb_model.pkl - Gradient Boosting model\")\n",
    "\n",
    "print(\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"  Total Corners Analyzed: {len(master_corners):,}\")\n",
    "print(f\"  Unique Drivers: {master_corners['vehicle_id'].nunique()}\")\n",
    "print(f\"  Tracks: {master_corners['track'].nunique()}\")\n",
    "print(f\"  Total Comparisons: {len(comparison_df):,}\")\n",
    "print(f\"  ML Training Samples: {len(ml_data)}\")\n",
    "\n",
    "print(\"\\n‚úÖ ALL DATA PROCESSING COMPLETE!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Update app.py with multi-track support\")\n",
    "print(\"  2. Test dashboard with new data\")\n",
    "print(\"  3. Add advanced visualizations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec7e6dc3-27e4-4a45-895d-fba741c9b312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Barber telemetry parameters...\n",
      "Sebring columns: ['meta_source', 'meta_time', 'meta_event', 'meta_session', 'timestamp', 'vehicle_id', 'outing', 'lap', 'value', 'expire_at']\n",
      "\n",
      "Unique telemetry_name values:\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTIC CELL - Check Barber telemetry parameters\n",
    "print(\"Checking Barber telemetry parameters...\")\n",
    "barber_telem = pd.read_csv('sebring/Sebring/Race 2/sebring_telemetry_R2.csv', nrows=10000)\n",
    "print(f\"Sebring columns: {barber_telem.columns.tolist()}\")\n",
    "print(f\"\\nUnique telemetry_name values:\")\n",
    "# print(barber_telem['telemetry_name'].unique()[:20])  # Show first 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e67f7e-fa85-4a9a-a062-6f96cabd6a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
